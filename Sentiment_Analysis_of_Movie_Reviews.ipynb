{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Movie Reviews"
      ],
      "metadata": {
        "id": "aXaMwNp0_yKN"
      },
      "id": "aXaMwNp0_yKN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1: About Dataset\n",
        "This Dataset was downloaded from IMDb website, it consist of 50K rows.\n",
        "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews.\n",
        "\n",
        "IMDb lets users rate movies on a scale from 1 to 10. To label these reviews the curator of the data labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive. Reviews with 5 or 6 stars were left out."
      ],
      "metadata": {
        "id": "fiyh48K3BUqo"
      },
      "id": "fiyh48K3BUqo"
    },
    {
      "cell_type": "markdown",
      "id": "28e13c3b",
      "metadata": {
        "id": "28e13c3b"
      },
      "source": [
        "## 2: Reading Dataset into Google Colab\n",
        "For most of what we want to do in this walkthrough we’ll only need our reviews to be in a Python list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceeabd97",
      "metadata": {
        "id": "ceeabd97"
      },
      "outputs": [],
      "source": [
        "reviews_train = []\n",
        "for line in open('./data/full_train.txt', 'r', encoding=\"utf8\"):\n",
        "    reviews_train.append(line.strip())\n",
        "\n",
        "reviews_test = []\n",
        "for line in open('./data/full_test.txt', 'r', encoding=\"utf8\"):\n",
        "    reviews_test.append(line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c2ce2b",
      "metadata": {
        "id": "d5c2ce2b"
      },
      "source": [
        "## 3: Data Cleaning and Preprocessing\n",
        "The raw text is pretty messy for these reviews so before we can do any analytics we need to clean things up. Here’s one example:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09638f2",
      "metadata": {
        "id": "c09638f2"
      },
      "source": [
        "\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1a1f55",
      "metadata": {
        "id": "3b1a1f55"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
        "\n",
        "    return reviews\n",
        "\n",
        "reviews_train_clean = preprocess_reviews(reviews_train)\n",
        "reviews_test_clean = preprocess_reviews(reviews_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aa53b9",
      "metadata": {
        "id": "b6aa53b9"
      },
      "source": [
        "And this is what the same review looks like now:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb6858c",
      "metadata": {
        "id": "ffb6858c"
      },
      "source": [
        "\"this isnt the comedic robin williams nor is it the quirky insane robin williams of recent thriller fame this is a hybrid of the classic drama without over dramatization mixed with robins new love of the thriller but this isnt a thriller per se this is more a mystery suspense vehicle through which williams attempts to locate a sick boy and his keeper also starring sandra oh and rory culkin this suspense drama plays pretty much like a news report until williams character gets close to achieving his goal i must say that i was highly entertained though this movie fails to teach guide inspect or amuse it felt more like i was watching a guy williams as he was actually performing the actions from a third person perspective in other words it felt real and i was able to subscribe to the premise of the story all in all its worth a watch though its definitely not friday saturday night fare it rates a   from the fiend\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96cc0bd",
      "metadata": {
        "id": "d96cc0bd"
      },
      "source": [
        "**Vectorization**\n",
        "In order for this data to make sense to our machine learning algorithm we’ll need to convert each review to a numeric representation, which we call vectorization.\n",
        "\n",
        "The simplest form of this is to create one very large matrix with one column for every unique word in your corpus (where the corpus is all 50k reviews in our case). Then we transform each review into one row containing 0s and 1s, where 1 means that the word in the corpus corresponding to that column appears in that review. That being said, each row of the matrix will be very sparse (mostly zeros). This process is also known as one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1aeaac",
      "metadata": {
        "id": "bb1aeaac"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(reviews_train_clean)\n",
        "X = cv.transform(reviews_train_clean)\n",
        "X_test = cv.transform(reviews_test_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a5f9fc",
      "metadata": {
        "id": "66a5f9fc"
      },
      "source": [
        "## 4: Build Classifier\n",
        "Logistic Regression is a good baseline model for us to use for several reasons: (1) They’re easy to interpret, (2) linear models tend to perform well on sparse datasets like this one, and (3) they learn very fast compared to other algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec031bc0",
      "metadata": {
        "id": "ec031bc0",
        "outputId": "053f0fd2-87ba-41d5-aeab-f141b6917936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.86976\n",
            "Accuracy for C=0.05: 0.87728\n",
            "Accuracy for C=0.25: 0.87808\n",
            "Accuracy for C=0.5: 0.87648\n",
            "Accuracy for C=1: 0.87504\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "\n",
        "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy for C=%s: %s\"\n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "\n",
        "#     Accuracy for C=0.01: 0.87472\n",
        "#     Accuracy for C=0.05: 0.88368\n",
        "#     Accuracy for C=0.25: 0.88016\n",
        "#     Accuracy for C=0.5: 0.87808\n",
        "#     Accuracy for C=1: 0.87648"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12eea40",
      "metadata": {
        "id": "e12eea40"
      },
      "source": [
        "It looks like the value of C that gives us the highest accuracy is 0.05."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14617600",
      "metadata": {
        "id": "14617600"
      },
      "source": [
        "## 5: Train Model\n",
        "Now i will train a model using the entire training set and evaluate accuracy on the 25k test reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85ca6c8",
      "metadata": {
        "id": "c85ca6c8",
        "outputId": "b772ae6f-8c80-49fe-823e-a256f127a44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Accuracy: 0.88156\n"
          ]
        }
      ],
      "source": [
        "final_model = LogisticRegression(C=0.05, solver='lbfgs', max_iter=1000)\n",
        "final_model.fit(X, target)\n",
        "print (\"Final Accuracy: %s\"\n",
        "       % accuracy_score(target, final_model.predict(X_test)))\n",
        "# Final Accuracy: 0.88128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983e2b17",
      "metadata": {
        "id": "983e2b17"
      },
      "source": [
        "As a sanity check, let’s look at the 5 most discriminating words for both positive and negative reviews. I will do this by looking at the largest and smallest coefficients, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767f9a0f",
      "metadata": {
        "id": "767f9a0f",
        "outputId": "78128fec-9ce0-43af-a254-7117dd77cbdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('excellent', 0.9287972580642614)\n",
            "('perfect', 0.7916720051806639)\n",
            "('great', 0.674056645967301)\n",
            "('amazing', 0.613182651568618)\n",
            "('superb', 0.6011369199241123)\n",
            "('worst', -1.3647113537043942)\n",
            "('waste', -1.166858301194453)\n",
            "('awful', -1.0320826529832108)\n",
            "('poorly', -0.8752032388463086)\n",
            "('boring', -0.8567748851705197)\n"
          ]
        }
      ],
      "source": [
        "feature_to_coef = {\n",
        "    word: coef for word, coef in zip(\n",
        "        cv.get_feature_names_out(), final_model.coef_[0]\n",
        "    )\n",
        "}\n",
        "for best_positive in sorted(\n",
        "    feature_to_coef.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True)[:5]:\n",
        "    print (best_positive)\n",
        "\n",
        "#     ('excellent', 0.9288812418118644)\n",
        "#     ('perfect', 0.7934641227980576)\n",
        "#     ('great', 0.675040909917553)\n",
        "#     ('amazing', 0.6160398142631545)\n",
        "#     ('superb', 0.6063967799425831)\n",
        "\n",
        "for best_negative in sorted(\n",
        "    feature_to_coef.items(),\n",
        "    key=lambda x: x[1])[:5]:\n",
        "    print (best_negative)\n",
        "\n",
        "#     ('worst', -1.367978497228895)\n",
        "#     ('waste', -1.1684451288279047)\n",
        "#     ('awful', -1.0277001734353677)\n",
        "#     ('poorly', -0.8748317895742782)\n",
        "#     ('boring', -0.8587249740682945)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ffdc66",
      "metadata": {
        "id": "b8ffdc66"
      },
      "source": [
        "And there it is. A very simple classifier with pretty decent accuracy out of the box."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41670f09",
      "metadata": {
        "id": "41670f09"
      },
      "source": [
        "## 6: Text Processing\n",
        "In the first iteration i did very basic text processing like removing punctuation and HTML tags and making everything lower-case. now i will clean things up further by removing stop words and normalizing the text.\n",
        "\n",
        "To make these transformations i will use libraries from the Natural Language Toolkit (NLTK). This is a very popular NLP library for Python."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85db7589",
      "metadata": {
        "id": "85db7589"
      },
      "source": [
        "**Removing Stop Words**\n",
        "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "296b365a",
      "metadata": {
        "id": "296b365a",
        "outputId": "26b72d85-9677-4271-84ae-b80e39aa3747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Mayank\n",
            "[nltk_data]     Monani\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stop_words = stopwords.words('english')\n",
        "def remove_stop_words(corpus):\n",
        "    removed_stop_words = []\n",
        "    for review in corpus:\n",
        "        removed_stop_words.append(\n",
        "            ' '.join([word for word in review.split()\n",
        "                      if word not in english_stop_words])\n",
        "        )\n",
        "    return removed_stop_words\n",
        "\n",
        "no_stop_words = remove_stop_words(reviews_train_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e43525",
      "metadata": {
        "id": "d1e43525"
      },
      "source": [
        "**Before**\n",
        "\n",
        "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell high’s satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers’ pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i’m here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn’t\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63df99cf",
      "metadata": {
        "id": "63df99cf"
      },
      "source": [
        "**After**\n",
        "\n",
        "\"bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead believe bromwell high's satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers' pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector i'm sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdeb0c47",
      "metadata": {
        "id": "fdeb0c47"
      },
      "source": [
        "**Normalization**\n",
        "A common next step in text preprocessing is to normalize the words in your corpus by trying to convert all of the different forms of a given word into one. Two methods that exist for this are Stemming and Lemmatization.\n",
        "\n",
        "**Stemming**\n",
        "\n",
        "Stemming is considered to be the more crude/brute-force approach to normalization (although this doesn’t necessarily mean that it will perform worse). There’s several algorithms, but in general they all use basic rules to chop off the ends of words.\n",
        "\n",
        "NLTK has several stemming algorithm implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d027d2bf",
      "metadata": {
        "id": "d027d2bf"
      },
      "outputs": [],
      "source": [
        "def get_stemmed_text(corpus):\n",
        "    from nltk.stem.porter import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
        "\n",
        "stemmed_reviews = get_stemmed_text(reviews_train_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e342f52",
      "metadata": {
        "id": "6e342f52"
      },
      "source": [
        "**Lemmatization**\n",
        "\n",
        "Lemmatization works by identifying the part-of-speech of a given word and then applying more complex rules to transform the word into its true root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d364cd",
      "metadata": {
        "id": "c1d364cd"
      },
      "outputs": [],
      "source": [
        "def get_lemmatized_text(corpus):\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
        "\n",
        "lemmatized_reviews = get_lemmatized_text(reviews_train_clean)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b618c3",
      "metadata": {
        "id": "a1b618c3"
      },
      "source": [
        "**Results**\n",
        "\n",
        "**No Normalization**\n",
        "\n",
        "\"this is not the typical mel brooks film it was much less slapstick than most of his movies and actually had a plot that was followable leslie ann warren made the movie she is such a fantastic under rated actress there were some moments that could have been fleshed out a bit more and some scenes that could probably have been cut to make the room to do so but all in all this is worth the price to rent and see it the acting was good overall brooks himself did a good job without his characteristic speaking to directly to the audience again warren was the best actor in the movie but fume and sailor both played their parts well\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b247e300",
      "metadata": {
        "id": "b247e300"
      },
      "source": [
        "**Stemmed**\n",
        "\n",
        "\"thi is not the typic mel brook film it wa much less slapstick than most of hi movi and actual had a plot that wa follow lesli ann warren made the movi she is such a fantast under rate actress there were some moment that could have been flesh out a bit more and some scene that could probabl have been cut to make the room to do so but all in all thi is worth the price to rent and see it the act wa good overal brook himself did a good job without hi characterist speak to directli to the audienc again warren wa the best actor in the movi but fume and sailor both play their part well\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e951e9",
      "metadata": {
        "id": "a4e951e9"
      },
      "source": [
        "**Lemmatized**\n",
        "\n",
        "\"this is not the typical mel brook film it wa much le slapstick than most of his movie and actually had a plot that wa followable leslie ann warren made the movie she is such a fantastic under rated actress there were some moment that could have been fleshed out a bit more and some scene that could probably have been cut to make the room to do so but all in all this is worth the price to rent and see it the acting wa good overall brook himself did a good job without his characteristic speaking to directly to the audience again warren wa the best actor in the movie but fume and sailor both played their part well\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee5610a",
      "metadata": {
        "id": "0ee5610a"
      },
      "source": [
        "**n-grams**\n",
        "Now i add more predictive power to this model by adding two or three word sequences (bigrams or trigrams) as well. For example, if a review had the three word sequence “didn’t love movie” it would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review.\n",
        "\n",
        "The scikit-learn library makes this really easy to play around with. Just use the ngram_range argument with any of the ‘Vectorizer’ classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd1dd5e",
      "metadata": {
        "id": "9fd1dd5e",
        "outputId": "f94a8468-9622-46f3-a823-0f6fa593673b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.88384\n",
            "Accuracy for C=0.05: 0.89056\n",
            "Accuracy for C=0.25: 0.8912\n",
            "Accuracy for C=0.5: 0.892\n",
            "Accuracy for C=1: 0.89152\n",
            "Final Accuracy: 0.89776\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "\n",
        "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy for C=%s: %s\"\n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "\n",
        "# Accuracy for C=0.01: 0.88416\n",
        "# Accuracy for C=0.05: 0.892\n",
        "# Accuracy for C=0.25: 0.89424\n",
        "# Accuracy for C=0.5: 0.89456\n",
        "# Accuracy for C=1: 0.8944\n",
        "\n",
        "final_ngram = LogisticRegression(C=0.5, solver='lbfgs', max_iter=1000)\n",
        "final_ngram.fit(X, target)\n",
        "print (\"Final Accuracy: %s\"\n",
        "       % accuracy_score(target, final_ngram.predict(X_test)))\n",
        "\n",
        "# Final Accuracy: 0.898"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89bf1b7f",
      "metadata": {
        "id": "89bf1b7f"
      },
      "source": [
        "Getting pretty close to 90%! So, simply considering 2-word sequences in addition to single words increased our accuracy by more than 1.6 percentage points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee89848e",
      "metadata": {
        "id": "ee89848e"
      },
      "source": [
        "## 7: Representations\n",
        "\n",
        "While this simple approach can work very well, there are ways that we can encode more information into the vector.\n",
        "\n",
        "**Word Counts**\n",
        "Instead of simply noting whether a word appears in the review or not, we can include the number of times a given word appears. This can give our sentiment classifier a lot more predictive power. For example, if a movie reviewer says ‘amazing’ or ‘terrible’ multiple times in a review it is considerably more probable that the review is positive or negative, respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457f0f7d",
      "metadata": {
        "id": "457f0f7d",
        "outputId": "98da8b52-4e22-42fb-e3db-3ad632d34246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.87152\n",
            "Accuracy for C=0.05: 0.88096\n",
            "Accuracy for C=0.25: 0.88144\n",
            "Accuracy for C=0.5: 0.87792\n",
            "Accuracy for C=1: 0.87584\n",
            "Final Accuracy: 0.8822\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wc_vectorizer = CountVectorizer(binary=False)\n",
        "wc_vectorizer.fit(reviews_train_clean)\n",
        "X = wc_vectorizer.transform(reviews_train_clean)\n",
        "X_test = wc_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75,\n",
        ")\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "\n",
        "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy for C=%s: %s\"\n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "\n",
        "# Accuracy for C=0.01: 0.87456\n",
        "# Accuracy for C=0.05: 0.88016\n",
        "# Accuracy for C=0.25: 0.87936\n",
        "# Accuracy for C=0.5: 0.87936\n",
        "# Accuracy for C=1: 0.87696\n",
        "\n",
        "final_wc = LogisticRegression(C=0.05, solver='lbfgs', max_iter=1000)\n",
        "final_wc.fit(X, target)\n",
        "print (\"Final Accuracy: %s\"\n",
        "       % accuracy_score(target, final_wc.predict(X_test)))\n",
        "\n",
        "# Final Accuracy: 0.88184"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06567ddf",
      "metadata": {
        "id": "06567ddf"
      },
      "source": [
        "**TF-IDF**\n",
        "Another common way to represent each document in a corpus is to use the tf-idf statistic (term frequency-inverse document frequency) for each word, which is a weighting factor that we can use in place of binary or word count representations.\n",
        "\n",
        "There are several ways to do tf-idf transformation but in a nutshell, tf-idf aims to represent the number of times a given word appears in a document (a movie review in our case) relative to the number of documents in the corpus that the word appears in — where words that appear in many documents have a value closer to zero and words that appear in less documents have values closer to 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb2e2b5",
      "metadata": {
        "id": "8eb2e2b5",
        "outputId": "3facb833-b8ca-4756-ff69-12ff8ff85fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.80128\n",
            "Accuracy for C=0.05: 0.83568\n",
            "Accuracy for C=0.25: 0.8744\n",
            "Accuracy for C=0.5: 0.88624\n",
            "Accuracy for C=1: 0.89328\n",
            "Final Accuracy: 0.8824\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(reviews_train_clean)\n",
        "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
        "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "\n",
        "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy for C=%s: %s\"\n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "\n",
        "# Accuracy for C=0.01: 0.79632\n",
        "# Accuracy for C=0.05: 0.83168\n",
        "# Accuracy for C=0.25: 0.86768\n",
        "# Accuracy for C=0.5: 0.8736\n",
        "# Accuracy for C=1: 0.88432\n",
        "\n",
        "final_tfidf = LogisticRegression(C=1, solver='lbfgs', max_iter=1000)\n",
        "final_tfidf.fit(X, target)\n",
        "print (\"Final Accuracy: %s\"\n",
        "       % accuracy_score(target, final_tfidf.predict(X_test)))\n",
        "\n",
        "# Final Accuracy: 0.882"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36031d21",
      "metadata": {
        "id": "36031d21"
      },
      "source": [
        "## 8: Algorithms\n",
        "\n",
        "**Support Vector Machines (SVM)**\n",
        "Recall that linear classifiers tend to work well on very sparse datasets (like the one we have). Another algorithm that can produce great results with a quick training time are Support Vector Machines with a linear kernel.\n",
        "\n",
        "Here’s an example with an n-gram range from 1 to 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e91ad5",
      "metadata": {
        "id": "43e91ad5",
        "outputId": "9eed837f-6176-4085-8d5c-939b526301b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.8944\n",
            "Accuracy for C=0.05: 0.8904\n",
            "Accuracy for C=0.25: 0.88896\n",
            "Accuracy for C=0.5: 0.8888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=1: 0.8888\n",
            "Final Accuracy: 0.89708\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "\n",
        "    svm = LinearSVC(C=c, max_iter=1500)\n",
        "    svm.fit(X_train, y_train)\n",
        "    print (\"Accuracy for C=%s: %s\"\n",
        "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
        "\n",
        "# Accuracy for C=0.01: 0.89104\n",
        "# Accuracy for C=0.05: 0.88736\n",
        "# Accuracy for C=0.25: 0.8856\n",
        "# Accuracy for C=0.5: 0.88608\n",
        "# Accuracy for C=1: 0.88592\n",
        "\n",
        "final_svm_ngram = LinearSVC(C=0.01, max_iter=1500)\n",
        "final_svm_ngram.fit(X, target)\n",
        "print (\"Final Accuracy: %s\"\n",
        "       % accuracy_score(target, final_svm_ngram.predict(X_test)))\n",
        "\n",
        "# Final Accuracy: 0.8974"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d263fc2",
      "metadata": {
        "id": "0d263fc2"
      },
      "source": [
        "## 9: Final Model\n",
        "\n",
        "I found that removing a small set of stop words along with an n-gram range from 1 to 3 and a linear support vector classifier gave me the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697745f7",
      "metadata": {
        "id": "697745f7",
        "outputId": "65d8358d-e4f8-4531-8f67-82e5273710cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.001: 0.87904\n",
            "Accuracy for C=0.005: 0.8872\n",
            "Accuracy for C=0.01: 0.89104\n",
            "Accuracy for C=0.05: 0.89088\n",
            "Accuracy for C=0.1: 0.8912\n",
            "Final Accuracy: 0.90024\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "for c in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
        "\n",
        "    svm = LinearSVC(C=c, max_iter=1000)\n",
        "    svm.fit(X_train, y_train)\n",
        "    print (\"Accuracy for C=%s: %s\"\n",
        "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
        "\n",
        "# Accuracy for C=0.001: 0.88784\n",
        "# Accuracy for C=0.005: 0.89456\n",
        "# Accuracy for C=0.01: 0.89376\n",
        "# Accuracy for C=0.05: 0.89264\n",
        "# Accuracy for C=0.1: 0.8928\n",
        "\n",
        "final = LinearSVC(C=0.01, max_iter=1000)\n",
        "final.fit(X, target)\n",
        "print (\"Final Accuracy: %s\"\n",
        "       % accuracy_score(target, final.predict(X_test)))\n",
        "\n",
        "# Final Accuracy: 0.90064\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f221a9",
      "metadata": {
        "id": "78f221a9"
      },
      "source": [
        "We broke the 90% mark!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a27ace",
      "metadata": {
        "id": "31a27ace"
      },
      "source": [
        "**Summary**\n",
        "I gone over several options for transforming text that can improve the accuracy of an NLP model. Which combination of these techniques will yield the best results will depend on the task, data representation, and algorithms choosen."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}